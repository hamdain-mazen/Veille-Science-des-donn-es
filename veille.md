#       **Veille Sciences des données**



# 1 Notions mathématiques

- ## Un vecteur et une matrice:
     En mathématiques, les matrices sont des tableaux d'éléments (nombres, caractères) qui servent à interpréter en termes calculatoires, et donc opérationnels, les résultats théoriques de l'algèbre linéaire et même de l'algèbre bilinéaire.[1]
    
    Un vecteur est un segment de droite orienté, formant un être mathématique sur lequel on peut effectuer des opérations.[2]




- ## Variables indépendantes
  Une variable indépendante dans un problème est un paramètre du problème qui varie sans être influencé par les autres paramètres du problème. Cela correspond le plus souvent aux paramètres exogènes ou imposés par la nature. Par exemple, dans un problème de mécanique non relativiste, le temps et les coordonnées spatiales sont des variables indépendantes. Leur évolution influence par contre celle des variables dépendantes du problème. [3]
- ## Une probabilité.

     La probabilité d'un événement est un nombre réel compris entre 0 et 1. Plus ce nombre est grand, plus le risque, ou la chance, que l'événement se produise est grand.[4]
- ## Espérance
      La valeur que l'on s'attend à trouver, en moyenne, si l'on répète un grand nombre de fois la même expérience aléatoire. Elle se note {E}(X) et se lit « espérance de X ».[5]
- ## Variance
   la variance est une mesure de la dispersion des valeurs d'un échantillon ou d'une distribution de probabilité. Elle exprime la moyenne des carrés des écarts à la moyenne, aussi égale à la différence entre la moyenne des carrés des valeurs de la variable et le carré de la moyenne, selon le théorème de König-Huygens. Ainsi, plus l'écart à la moyenne est grand plus il est prépondérant dans le calcul total (voir la fonction carré) de la variance qui donnerait donc une bonne idée sur la dispersion des valeurs. [6]

 - ## Écart-Type
     En mathématiques, l’écart type est une mesure de la dispersion des valeurs d'un échantillon statistique ou d'une distribution de probabilité. Il est défini comme la racine carrée de la variance ou, de manière équivalente, comme la moyenne quadratique des écarts par rapport à la moyenne. [7]


- ## Une corrélation linéaire
     La corrélation est une mesure statistique qui exprime la notion de liaison linéaire entre deux variables (ce qui veut dire qu'elles évoluent ensemble à une vitesse constante). C'est un outil courant permettant de décrire des relations simples sans s'occuper de la cause et de l'effet.[8]


- ## Une moyenne
     la somme des termes de la liste, divisée par le nombre de termes.[9]
- ## une médiane
     la valeur centrale d'une série statistique dont les valeurs observées ont été rangées dans l'ordre croissant, est la valeur qui partage la population étudiée en deux sous-ensembles de même effectif (si le nombre d'observations n est pair, la médiane est la demi-somme des termes de rang n et n + 1).
     Exemple : La médiane de la série : 4, 1, et 7 est 4 car, lorsqu'on ordonne les valeurs de la série dans l'ordre croissant (1, 4, 7), 4 est la valeur qui divise la série en deux moitiés égales.[10]
- ## un maximum
     La valeur « maximum » en statistique est la plus grande valeur que l'on retrouve dans une population.[11]
- ## un minimum
     La valeur « minimum » en statistique est la plus petite valeur que l’on retrouve dans une population.
- ## Les quantiles en statistique
     sont les valeurs qui divisent un jeu de données en intervalles de même probabilité égale. Il y a donc un quantile de moins que le nombre de groupes créés. Par exemple, les quartiles sont les trois quantiles qui divisent un ensemble de données en quatre groupes de même probabilité. La médiane quant à elle est le quantile qui sépare le jeu de données en deux groupes de même probabilité.

- ##  Boxplot et Histogramme
    un boxplot, également connu sous le nom de box and whisker plot, est un type de graphique souvent utilisé dans l'analyse de données. Les diagrammes en boîte montrent visuellement la distribution des données numériques et l'asymétrie en affichant les quartiles et les moyennes des données.[13]
    Les histogrammes sont utilisés pour les données continues, tandis que les diagrammes en barres sont utilisés pour les données catégorielles ou nominales. Les histogrammes ne contiennent pas d'écart entre les barres. Les barres représentent le nombre de valeurs se situant dans une étendue spécifiée sur l'axe horizontal. [14]
- ## Fonction de coût.
     Définition : une fonction de prix peut être une formule mathématique qui permet de déterminer comment les dépenses de production vont évoluer à différents niveaux de production. En d’autres termes, elle estime le coût total de production en fonction d’une quantité sélectionnée produite.

    




- ## Une dérivée.
    La dérivée d'une fonction d'une variable réelle mesure l'ampleur du changement de la valeur de la fonction (valeur de sortie) par rapport à un petit changement de son argument (valeur d'entrée). Les calculs de dérivées sont un outil fondamental du calcul infinitésimal. Par exemple, la dérivée de la position d'un objet en mouvement par rapport au temps est la vitesse (instantanée) de l'objet.
- ## Descente de gradient.
    La Descente de Gradient, (ou Gradient Descent en anglais) est un des algorithmes les plus importants de tout le Machine Learning et de tout le Deep Learning. il s’agit d’un algorithme d’optimisation extrêmement puissant qui permet d’entraîner les modèles de régression linéaire, régression logistiques ou encore les réseaux de neurones. Si vous vous lancez dans le Machine Learning, il est donc impératif de comprendre en profondeur l’algorithme de la descente de gradient.[15]
- ## Équation normale.
     En statistique, les équations normales sont des équations matricielles de la forme :

    tAAx = tAb
    où

    A est une matrice de réels de dimensions n×p ;
    tA est la matrice transposée de A ;
    x est un vecteur réel inconnu de dimension p ;
    b est un vecteur connu de dimension n.
- ## La loi Normale.
     Les lois normales ont une grande importance en statistiques. La courbe représentative de leur fonction de densité est appelée courbe de Gauss ou courbe en cloche du fait de sa forme. Elle possède un axe de symétrie en la moyenne ou la médiane (elles sont égales) et des intervalles remarquables (68% des observations sont comprises dans un intervalle de +/- un fois l'écart-type autour de la moyenne.). [16]
- ## Théorème Centrale Limite.
    Le théorème central limite établit la convergence en loi de la somme d'une suite de variables aléatoires vers la loi normale. Intuitivement, ce résultat affirme qu'une somme de variables aléatoires indépendantes et identiquement distribuées tend vers une variable aléatoire gaussienne.
- ## Un test d'hypothèse.
    Est une procédure de décision entre deux hypothèses. Il s'agit d'une démarche consistant à rejeter ou à ne pas rejeter une hypothèse statistique, appelée hypothèse nulle, en fonction d'un échantillon de données.
- ## Chi-Square test.
    En statistique, le test du khi carré, aussi dit du khi-deux1, d’après sa désignation symbolique χ2, est un test statistique où la statistique de test suit une loi du χ2 sous l'hypothèse nulle.

    Par exemple, il permet de tester l'adéquation d'une série de données à une famille de lois de probabilité ou de tester l'indépendance entre deux variables aléatoires.
- ## ANOVA.
    L’analyse de la variance (ANOVA) peut déterminer si les moyennes de trois groupes ou plus sont différentes. ANOVA utilise des tests F pour tester statistiquement l’égalité des moyennes.[17] 
- ## Une valeur propre.
    L'ensemble des saturations des variables pour une composante constitue un vecteur propre. La valeur propre (ou "eigenvalue") est la somme des carrés de ces saturations. Elle représente la quantité de variance du nuage de points expliquée par cette composante.

    Le rapport de la valeur propre au nombre de variables soumises à l'analyse donne le pourcentage de variance expliquée par la composante (taux d'inertie).
    [18] 

# 2 Outils et librairies

- ##  Anaconda 
    Anaconda est une distribution libre et open source2 des langages de programmation Python et R appliqué au développement d'applications dédiées à la science des données et à l'apprentissage automatique (traitement de données à grande échelle, analyse prédictive, calcul scientifique), qui vise à simplifier la gestion des paquets et de déploiement3. Les versions de paquetages sont gérées par le système de gestion de paquets conda.

- ## Jupyter-notebook 
    Jupyter Notebook (anciennement IPython Notebooks) est un environnement de programmation interactif basé sur le Web permettant de créer des documents Jupyter Notebook. Le terme "notebook" peut faire référence à de nombreuses entités différentes, adaptées au contexte, telles que l'application web Jupyter, le serveur web Jupyter Python ou le format de document Jupyter.Un document Jupyter Notebook est un document JSON. Il suit un schéma contenant une liste ordonnée de cellules d'entrée/sortie. Celles-ci peuvent contenir du code, du texte (à l'aide de Markdown), des formules mathématiques, des graphiques et des médias interactifs. Ce document se termine généralement par l'extension ".ipynb".
- ## Librairies Python : Pandas, NumPy, Seaborn, Matplotlib, Plotly, Scikit-Learn, StatsModels, nltk, Pycaret.
    Pandas : Pandas est une librairie python qui permet de manipuler facilement des données à analyser :manipuler des tableaux de données avec des étiquettes de variables (colonnes) et d'individus (lignes).ces tableaux sont appelés DataFrames, similaires aux dataframes sous R.on peut facilement lire et écrire ces dataframes à partir ou vers un fichier tabulé.on peut faciler tracer des graphes à partir de ces DataFrames grâce à matplotlib.Pour utiliser pandas : import pandas

    #### NumPy: 
    NumPy est une bibliothèque pour langage de programmation Python, destinée à manipuler des matrices ou tableaux multidimensionnels ainsi que des fonctions mathématiques opérant sur ces tableaux.

    Plus précisément, cette bibliothèque logicielle libre et open source fournit de multiples fonctions permettant notamment de créer directement un tableau depuis un fichier ou au contraire de sauvegarder un tableau dans un fichier, et manipuler des vecteurs, matrices et polynômes.

    NumPy est la base de SciPy, regroupement de bibliothèques Python autour du calcul scientifique.
    #### Seaborn: 
    Seaborn est une bibliothèque permettant de créer des graphiques statistiques en Python. Elle est basée sur Matplotlib, et s'intègre avec les structures Pandas.[19]

    #### Matplotlib 
    Est une bibliothèque du langage de programmation Python destinée à tracer et visualiser des données sous forme de graphiques. Elle peut être combinée avec les bibliothèques python de calcul scientifique NumPy et SciPy.[20]

    #### Plotly: 
    est une bibliothèque open source qui peut être utilisée pour visualiser et comprendre les données simplement et facilement. Plotly prend en charge différents types de tracés tels que les graphiques linéaires, les nuages ​​de points, les histogrammes, les tracés de Cox, etc. https://stacklima.com/tutoriel-python-plotly/


    #### Scikit-Learn : 
    Scikit-learn est une bibliothèque libre Python destinée à l'apprentissage automatique. Elle est développée par de nombreux contributeurs notamment dans le monde académique par des instituts français d'enseignement supérieur et de recherche comme Inria.

    https://fr.wikipedia.org/wiki/Scikit-learn

    #### StatsModels:
    est un package Python qui permet aux utilisateurs d'explorer des données, d'estimer des modèles statistiques et d'effectuer des tests statistiques.

    https://en.wikipedia.org/wiki/Statsmodels

    #### nltk: 
    est une bibliothèque logicielle en Python permettant un traitement automatique des langues, développée par Steven Bird et Edward Loper du département d'informatique de l'université de Pennsylvanie. En plus de la bibliothèque, NLTK fournit des démonstrations graphiques, des données-échantillon, des tutoriels, ainsi que la documentation de l'interface de programmation (API).

    https://fr.wikipedia.org/wiki/Natural_Language_Toolkit

    #### Pycaret:
    PyCaret est une bibliothèque open source à faible code en python conçue pour automatiser le développement de modèles d’apprentissage automatique. Cette bibliothèque s’adresse notamment aux data scientists, aux ingénieurs Machine Learning, mais aussi aux apprentis souhaitant être plus productif et souhaitant obtenir plus rapidement des conclusions.

    https://www.lebigdata.fr/pycaret-tout-savoir







- ## Librairies R : dplyr, ggplot2, tidyr, tidyverse, Shiny, plotly, Caret.
    #### dplyr : 
    Traduit de l'anglais
    L'un des packages de base du tidyverse dans le langage de programmation R, dplyr est principalement un ensemble de fonctions conçues pour permettre la manipulation de trames de données de manière intuitive et conviviale.

    #### ggplot2
    ggplot2 est une librairie R de visualisation de données développée par Hadley Wickham. La librairie est développée selon les principes développés par Leland Wilkinson dans son ouvrage The Grammar of Graphics.

    #### tidyr
    est un paquet de R faisant partie de l'écosystème Tidyverse qui permet d'organiser des données afin de faciliter la manipulation, la visualisation, ou la modélisation. Dans cet article, le terme organisation des données est une traduction limitative du correspondant tidy en anglais. Par organisation il faut en effet entendre plus en général les différentes actions qui permettent de préparer un ou plusieurs jeu de données : nettoyer les données, structurer les variables/colonnes, exclure les données manquantes, etc.[22]

    #### Shiny 
    est un package R qui permet la création simple d’applications web intéractives depuis R

    #### plotly

    Plotly R  est une graphing librarie interactive,  publication-qualité graphs.[23]
    #### caret
    C'est un package qui permet d'appeler de nombreuses méthodes de machine learning en offrant une interface unifiée et qui comporte des fonctions utilitaires diverses.

    http://www.duclert.org/r-apprentissage/caret-R.php#:~:text=C'est%20un%20package%20qui,comporte%20des%20fonctions%20utilitaires%20diverses



- ## Définition d'un ETL et exemples.
    Un ETL est un outil qui collecte et traite (presque en temps réel) des données provenant de diverses sources pour ensuite les envoyer dans un espace de stockage (data lake, data warehouse) à des fins d’analyse et de Business Intelligence. 

    Un outil d’ETL permet de gagner un temps considérable sur l’extraction et la préparation des données. Il permet également de réduire le risque d’erreur humaine. 

    Aujourd’hui, avec les quantités de données qu’une entreprise génère, un ETL demeure essentiel pour mener à bien l’analyse de données business et prendre de meilleures décisions stratégiques au jour le jour (sans perdre trop de temps). 

    Extract : extraction des données brutes depuis les différentes sources.

    Transforme : nettoyage et formatage des données.

    Load : transfert des données transformées vers un data warehouse / data lake.[21]


- ## Une base de données relationnelle.
    Une base de données relationnelle est un type de base de données où les données sont liées à d'autres informations au sein des bases de données. Les bases de données relationnelles sont composées d’un ensemble de tables qui peuvent être accessibles et reconstruites de différentes manières, sans qu'il soit nécessaire de réarranger ces tables de quelque façon que ce soit. [https://www.oracle.com/fr/database/base-de-donnees-relationnelle-definition.html ]
- ## Power BI et Tableau.
    Microsoft Power BI est une solution d'analyse de données de Microsoft. Il permet de créer des visualisations de données personnalisées et interactives avec une interface suffisamment simple pour que les utilisateurs finaux créent leurs propres rapports et tableaux de bord.

    Tableau Software est une société de logiciel américaine, partie du groupe Salesforce, dont le siège se trouve à Seattle. Elle conçoit une famille de produits orientés visualisation de données.

# 3 Apprentissage automatique
- ## La science des données.
    La science des données est l'étude de l’extraction automatisée de connaissance à partir de grands ensembles de données1,2.

    Plus précisément, la science des données est un domaine interdisciplinaire qui utilise des méthodes, des processus, des algorithmes et des systèmes scientifiques pour extraire des connaissances et des idées à partir de nombreuses données structurées ou non . Elle est souvent associée aux données massives et à l'analyse des données.
- ## L’apprentissage automatique.
    est un champ d'étude de l'intelligence artificielle qui se fonde sur des approches mathématiques et statistiques pour donner aux ordinateurs la capacité d'« apprendre » à partir de données, c'est-à-dire d'améliorer leurs performances à résoudre des tâches sans être explicitement programmés pour chacune. Plus largement, il concerne la conception, l'analyse, l'optimisation, le développement et l'implémentation de telles méthodes.

- ## L’apprentissage profond.
    est un ensemble de méthodes d'apprentissage automatique tentant de modéliser avec un haut niveau d’abstraction des données grâce à des architectures articulées de différentes transformations non linéaires. Ces techniques ont permis des progrès importants et rapides dans les domaines de l'analyse du signal sonore ou visuel et notamment de la reconnaissance faciale, de la reconnaissance vocale, de la vision par ordinateur, du traitement automatisé du langage. Dans les années 2000, ces progrès ont suscité des investissements privés, universitaires et publics importants, notamment de la part des GAFAM (Google, Apple, Facebook, Amazon, Microsoft).

- ## Deep vs Machine Learning (dans quel cas, on utilise l’un ou l’autre).
    Le Machine Learning est une IA capable de s'adapter automatiquement avec une interférence humaine minimale, et le Deep Learning est un sous-ensemble du Machine Learning utilisant les réseaux de neurones pour mimer le processus d'apprentissage du cerveau humain. [https://datascientest.com/quelle-difference-entre-le-machine-learning-et-deep-learning#:~:text=Le%20Machine%20Learning%20est%20une,d'apprentissage%20du%20cerveau%20humain.]
- ## La différence entre l'apprentissage supervisé et l'apprentissage non supervisé.
    Si votre base est étiquetée et que vous savez clairement dans quelles catégories vous souhaitez classer vos données, alors l'apprentissage supervisé est pour vous. Si vos données ne sont pas étiquetées et que le faire représenterait un coût trop important, alors optez pour l'apprentissage non supervisé. [https://experiences.microsoft.fr/articles/intelligence-artificielle/apprentissage-supervise-et-non-supervise-quelles-differences/#:~:text=Si%20votre%20base%20est%20%C3%A9tiquet%C3%A9e,pour%20l'apprentissage%20non%20supervis%C3%A9.]
- ## La classification et ses métriques d'évaluation.
    la classification consiste aussi à trouver le lien entre une variable X et une variable aléatoire discrète suivant une loi multinomiale Y.
    ses métriques : Precision, recall et F1 Score
- ## La régression et ses métriques d'évaluation.
    En statistiques ou en apprentissage automatique, la régression linéaire est l'un des algorithmes les plus connus et utilisés. Simple et très efficace, il permet de résoudre des problématiques de tous genres pour de multiples entités et secteurs. On met en avant le contexte d'utilisation de l'algorithme de régression linéaire, son fonctionnement et les cas concrets où il peut être utilisé en Machine Learning. 
    La MAE, ou erreur absolue moyenne, est la moyenne des valeurs absolues des erreurs.
    La Root Mean Squared Error (RMSE) et la Mean Squared Error (MSE) sont les métriques de régression les plus courantes. Du fait de leurs propriétés de régularité, ce sont les métriques historiques pour optimiser les modèles de régression comme la régression linéaire. [24]





- ## Le clustering et ses métriques de décision du k optimal et de la qualité des clusters.

    Le clustering sert principalement à segmenter ou classifier une base de données (par exemple trier des données clients type âge, profession exercée, lieu de résidence, etc., pour optimiser la gestion de la relation client) ou extraire des connaissances pour tenter de relever des sous-ensembles de données difficiles à identifier à l’œil nu.

    méthodes hiérarchiques
    Les méthodes de clustering de type hiérarchique sont différentes. Elles forment pas à pas des connexions entre individus (pour les méthodes de clustering hiérarchiques ascendantes), et utilisent une matrice de distances entre individus pour trouver le regroupement le plus proche d’un autre.


    méthodes centroïdes
    La méthode centroïde la plus classique est la méthode des k-moyennes. Elle ne nécessite qu’un seul choix de départ : k, le nombre de classes voulues.
    On initialise l’algorithme avec k points au hasard parmi les n individus.

    méthodes à densité
    Les classes des méthodes à densité correspondent aux zones de densité relativement élevées, c’est-à-dire les zones où beaucoup de points sont proches par rapport à d’autres zones de l’espace R en dimension p (cf illustration).[25]






- ## Traitement automatique du langage.
    Le traitement automatique de la langue naturelle ou traitement automatique des langues, plus couramment appelé NLP est un domaine multidisciplinaire impliquant la linguistique, l'informatique et l'intelligence artificielle, qui vise à créer des outils de traitement de la langue naturelle pour diverses applications.[26]



- ## La réduction de dimensions.
    La réduction de la dimensionnalité (ou réduction de (la) dimension) est un processus étudié en mathématiques et en informatique, qui consiste à prendre des données dans un espace de grande dimension, et à les remplacer par des données dans un espace de plus petite dimension.[27]


- ## La features engineering.

    La Feature Engineering a un rôle important, notamment dans l’analyse des données. Sans données, les algorithmes d’exploitation et d’apprentissage automatique de données ne seront pas en mesure de fonctionner. En effet, il s’avère qu’en réalité, on ne pourrait réaliser que peu de choses si nous disposons que de très peu de caractéristiques afin de pouvoir représenter les objets de données, ou les banques de données, sous-jacents. De surcroît, la qualité des résultats de tous ces algorithmes est dépendante et est directement liée à la qualité des caractéristiques dont on peut disposer. Ainsi, il existe différentes formes de données comme la séquence, la série chronologique, le graphique, le texte ou encore l’image.[28]


- ## La validation croisée, données d'entraînement, données de validation et données de test.
    La validation croisée (« cross-validation ») est, en apprentissage automatique, une méthode d’estimation de fiabilité d’un modèle fondée sur une technique d’échantillonnage. [29]

    En apprentissage automatique, une tâche courante est l'étude et la construction d'algorithmes qui peuvent apprendre et faire des prédictions sur les données1. De tels algorithmes fonctionnent en faisant des prédictions ou des décisions basées sur les données2, en construisant un modèle mathématique à partir des données d'entrée. Ces données d'entrée utilisées pour construire le modèle sont généralement divisées en plusieurs jeux de données . En particulier, trois jeux de données sont couramment utilisés à différentes étapes de la création du modèle : les jeux d'apprentissage, de validation et de test.[30] 









# 4 Bonus : Aspect métiers

- ## Définissez les métiers de Data Scientist, Data Analyst et Data Engineer. Donnez la différence entre les trois.
    Pour résumer la différence entre le data analyst vs data scientist, le premier (data analyst) sera capable d’extraire de données brutes à partir d’un existant (Big Data) pour en tirer des conclusions stratégiques à haute valeur ajoutée et développer des outils stratégiques et décisionnels à très forte valeur ajoutée. L’autre sera un spécialiste de l’analyse brutes de données et saura mettre en place des modèles prédictifs mathématiques et statistiques qui constitueront un outil décisionnel très recherché.

    Un Data Engineer est quelqu’un ayant un background technique en développement logiciel. Il peut être un Software Engineer qui s’est reconverti dans le Big Data.

    Les Data Engineers vont mettre en place des systèmes de Big Data pour traiter ces dernières. Ils opteront pour des outils de stockage performants comme les bases de données NoSQL et se baseront sur  Hadoop, Spark, Map/Reduce pour traiter convenablement ces grands volumes de données. [31]



# 5 Bibliographie: 
- #### [1] Matrice , Wikipedia
- #### [2] Vecteur , Wikipedia
- #### [3] Variable independante , Wikipedia
- #### [4] Probabilité , Wikipedia
- #### [5] Espérance , Wikipedia
- #### [6] Variance , Wikipedia
- #### [7] Écart-Type , Wikipedia
- #### [8] Une corrélation linéaire , Wikipedia
- #### [9] une moyenne , Khan Academy
- #### [10] une médiane , Khan Academy
- #### [11],[12] EcoChTermesStat.doc , https://www.pfi-culture.org/wp-content/uploads/sites/1052/2016/04/0ecochtermesstat.pdf


- #### Anaconda , Wikipedia 14/11/2022 https://fr.wikipedia.org/wiki/Anaconda_(distribution_Python)
- #### Jupyter Notebook, Wikipedia 14/11/2022  https://fr.wikipedia.org/wiki/Jupyter

- #### Pandas , http://www.python-simple.com/python-pandas/panda-intro.php
- #### [13] box plot, https://www.data-transitionnumerique.com/boite-moustache-boxplot/#:~:text=En%20statistique%20descriptive%2C%20un%20boxplot,et%20les%20moyennes%20des%20donn%C3%A9es.
- #### [14] Introdction aux statistiques https://www.jmp.com/fr_fr/statistics-knowledge-portal/exploratory-data-analysis/histogram.html#:~:text=Les%20histogrammes%20sont%20utilis%C3%A9s%20pour,sp%C3%A9cifi%C3%A9e%20sur%20l'axe%20horizontal.
- #### [15] https://machinelearnia.com/descente-de-gradient/
- #### [16] Khan academy,  Khan Academy, https://fr.khanacademy.org/math/be-6eme-secondaire2h2/xa29f433c00318f09:probabilites/xa29f433c00318f09:loi-binomiale/a/normal-distributions-review#:~:text=La%20fonction%20de%20densit%C3%A9%20de,et%20le%20point%20d'inflexion.
- #### [17] Anova,  https://blog.minitab.com/fr/comprendre-lanalyse-de-la-variance-anova-et-le-test-f
- #### [18] psychometrie.jlroulin http://psychometrie.jlroulin.fr/cours/aide_quizz.html?H33.html
- #### [19] datascientist, https://datascientest.com/seaborn
- #### [20] https://fr.wikipedia.org/wiki/Matplotlib
- #### [21] https://www.boryl.fr/glossaire/etl-extract-transform-load/#:~:text=L'ETL%20signifie%20l'Extraction,analyse%20et%20de%20Business%20Intelligence
- #### [22] https://edutechwiki.unige.ch/fr/Organiser_des_donn%C3%A9es_avec_tidyr

- #### [23] https://plotly.com/r/
- #### [24] https://kobia.fr/regression-metrics-quelle-metrique-choisir/
- #### [25] https://larevueia.fr/clustering-les-3-methodes-a-connaitre/
- #### [26] https://fr.wikipedia.org/wiki/Traitement_automatique_des_langues
- #### [27] https://fr.wikipedia.org/wiki/R%C3%A9duction_de_la_dimensionnalit%C3%A9#:~:text=La%20r%C3%A9duction%20de%20la%20dimensionnalit%C3%A9,espace%20de%20plus%20petite%20dimension.
- #### [28] https://fr.wikipedia.org/wiki/Ing%C3%A9nierie_des_caract%C3%A9ristiques
- #### [29] https://fr.wikipedia.org/wiki/Validation_crois%C3%A9e
- #### [30] https://fr.wikipedia.org/wiki/Jeux_d'entrainement,_de_validation_et_de_test
- #### [31] https://mrmint.fr/data-scientist-data-engineer-data-analyst-quelles-sont-les-differences-entre-ces-metiers





